{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jacfwd\n",
    "from utils_v2 import KalmanFilter\n",
    "from typing import Tuple\n",
    "import jax.random as jrandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedKalmanFilter(KalmanFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_0: jnp.ndarray | float | int,\n",
    "        f: callable,\n",
    "        h: callable,\n",
    "        R: jnp.ndarray,\n",
    "        Q: jnp.ndarray,\n",
    "        Z: jnp.ndarray,\n",
    "        w_k: jnp.ndarray,\n",
    "        P_0: jnp.ndarray,\n",
    "    ) -> None:\n",
    "        if not isinstance(x_0, (jnp.ndarray, float, int)):\n",
    "            raise Exception(\n",
    "                \"The State input must be of type jnp.ndarray, float, or int\"\n",
    "            )\n",
    "        # Expand scalar state into array if necessary\n",
    "        x_0 = jnp.expand_dims(x_0, axis=-1) if type(x_0) in (int, float) else x_0\n",
    "        super().__init__(x_0, None, None, None, None, R, Q, Z, w_k, P_0)\n",
    "        self.f = f  # Nonlinear state transition function: f(x, u)\n",
    "        self.h = h  # Nonlinear measurement function: h(x)\n",
    "\n",
    "    def _derivative(\n",
    "        self, f: callable, x: float | jnp.ndarray, h: float = 1e-5\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Compute finite-difference derivative for a scalar function.\n",
    "        If x is a jnp.ndarray, assumes single element.\n",
    "        \"\"\"\n",
    "        if isinstance(x, jnp.ndarray):\n",
    "            x = x[0]\n",
    "        return jnp.array((f(x + h) - f(x - h)) / (2 * h))\n",
    "\n",
    "    def _gradient(\n",
    "        self, f: callable, x: jnp.ndarray, u: jnp.ndarray = None\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of a scalar-valued function f at x.\n",
    "        Returns the gradient as a diagonal matrix.\n",
    "        \"\"\"\n",
    "        grad_f = grad(f)\n",
    "        # Wrap gradient vector as a diagonal matrix\n",
    "        return (\n",
    "            jnp.diag(jnp.array(grad_f(x)))\n",
    "            if u is None\n",
    "            else jnp.diag(jnp.array(grad_f(x, u)))\n",
    "        )\n",
    "\n",
    "    def _jacobian(\n",
    "        self, f: callable, x: jnp.ndarray, u: jnp.ndarray = None\n",
    "    ) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the full Jacobian of a vector-valued function f at x.\n",
    "        \"\"\"\n",
    "        jac_F = jacfwd(f)\n",
    "        return jnp.array(jac_F(x)) if u is None else jnp.array(jac_F(x, u))\n",
    "\n",
    "    def _set_matrices(self, u_k: jnp.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the matrices A and H by choosing the appropriate differentiation method\n",
    "        based on the initial state shape. Also stores the chosen function in _function.\n",
    "        \"\"\"\n",
    "        # Check if state is scalar-like: either a float/int or a 0-dim jnp.ndarray.\n",
    "        if type(self.x_k) in (float, int) or (\n",
    "            type(self.x_k) is jnp.ndarray and jnp.array(self.x_k).squeeze().shape == ()\n",
    "        ):\n",
    "            self.A = self._derivative(self.f, self.x_k)\n",
    "            self.H = self._derivative(self.h, self.x_k)\n",
    "            self._function = self._derivative\n",
    "        # If state is a vector (1-dimensional array)\n",
    "        elif self.x_k.squeeze().ndim == 1:\n",
    "            self.A = self._gradient(self.f, self.x_k, u_k)\n",
    "            self.H = self._gradient(self.h, self.x_k)\n",
    "            self._function = self._gradient\n",
    "        # Otherwise assume state is higher-dimensional and use the Jacobian\n",
    "        else:\n",
    "            self.A = self._jacobian(self.f, self.x_k, u_k)\n",
    "            self.H = self._jacobian(self.h, self.x_k)\n",
    "            self._function = self._jacobian\n",
    "\n",
    "    def _step_estimation(self, u_k: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts the next state using the nonlinear system dynamics f and control input u_k.\n",
    "\n",
    "        Args:\n",
    "            u_k: Control input vector (m x 1)\n",
    "\n",
    "        Returns:\n",
    "            Predicted state vector (n x 1)\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If an error occurs during the prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            new_x_k = self.f(self.x_k, u_k) + self.w_k\n",
    "            return new_x_k\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in EKF step estimation: {e}\") from e\n",
    "\n",
    "    def _current_state_and_process(\n",
    "        self, x_km: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Updates the state estimate using a measurement and the Kalman gain.\n",
    "\n",
    "        This function recomputes the linearization matrices A and H based on the current state.\n",
    "\n",
    "        Args:\n",
    "            x_km: Noisy measurement vector (p x 1)\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - Corrected state estimate (n x 1)\n",
    "            - Updated error covariance matrix (n x n)\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If an error occurs during state update.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Recompute linearization at current state\n",
    "            self.A = self._function(self.f, self.x_k)\n",
    "            self.H = self._function(self.h, self.x_k)\n",
    "            measurements = self.h(x_km) + self.Z\n",
    "            x_k = self.x_k + self.K @ (measurements - self.H @ self.x_k)\n",
    "            p_k = (jnp.eye(self.K.shape[0]) - self.K @ self.H) @ self.P\n",
    "            return x_k, p_k\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error in EKF state and process update: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_nonlinear(x, u):\n",
    "    return x + u\n",
    "\n",
    "\n",
    "def h_nonlinear(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "x0 = jnp.array([1.0, 2.0,3.1,4.8])\n",
    "delta_t = 0.1\n",
    "m = 0.1\n",
    "M = 1\n",
    "g = 9.8\n",
    "l = 0.5\n",
    "A = jnp.array(\n",
    "    [\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, m * g * delta_t / M, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 0, (m + M) * g * delta_t / (M * l), 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# A=jnp.array([[1,delta_t,0,0],\n",
    "#             [0,1,m*g*delta_t/M,0],\n",
    "#             [0,0,1,delta_t],\n",
    "#             [0,0,(m+M)*g*delta_t/(M*l),1]])\n",
    "\n",
    "B = jnp.array([[0], [delta_t / M], [0], [delta_t / (M * l)]])\n",
    "\n",
    "H = jnp.array([[0, 1, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "R = jnp.eye(H.shape[0]) * 0.005\n",
    "C = H\n",
    "Q = jnp.eye(A.shape[0]) * 0.005\n",
    "mean = 0\n",
    "std_dev = 0.005\n",
    "# Q=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean     #FAILED\n",
    "P_0 = jnp.ones(A.shape)\n",
    "key = jrandom.PRNGKey(42)\n",
    "Z = jrandom.normal(key, shape=(C.shape[0], 1)) * std_dev + mean\n",
    "w_k = jnp.ones((A.shape[0], 1)) * 0.005\n",
    "# w_k=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean    #FAILED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekf = ExtendedKalmanFilter(x0, f_nonlinear, h_nonlinear, R, Q, Z, w_k, P_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in the proccess covariance function, the error:  unsupported operand type(s) for @: 'NoneType' and 'jaxlib.xla_extension.ArrayImpl'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1.505    , 0.705    , 1.005    , 1.505    ],\n",
       "       [2.505    , 1.705    , 2.005    , 2.505    ],\n",
       "       [3.605    , 2.805    , 3.105    , 3.605    ],\n",
       "       [5.3050003, 4.505    , 4.8050003, 5.3050003]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekf.predict(jnp.array([0.5, -0.3,0,0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Gradient only defined for scalar-output functions. Output had shape: (2,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m grad_f \u001b[38;5;241m=\u001b[39m grad(F)  \u001b[38;5;66;03m# Computes the gradient function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m x0 \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Output: [4. 6.]\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jax\\_src\\api.py:492\u001b[0m, in \u001b[0;36m_check_scalar\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(aval, ShapedArray):\n\u001b[0;32m    491\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m aval\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m ():\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad abstract value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: Gradient only defined for scalar-output functions. Output had shape: (2,)."
     ]
    }
   ],
   "source": [
    "def F(x):\n",
    "    return jnp.array([x[0] ** 2 + x[1], x[0] + x[1] ** 2])\n",
    "\n",
    "\n",
    "grad_f = grad(F)  # Computes the gradient function\n",
    "x0 = jnp.array([2.0, 3.0])\n",
    "print(grad_f(x0))  # Output: [4. 6.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
