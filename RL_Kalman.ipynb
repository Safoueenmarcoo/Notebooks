{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import gymnasium as gym\n",
        "import jax.random as jrandom\n",
        "from stable_baselines3 import PPO\n",
        "from utils_v2 import KalmanFilter,KalmanRLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_t = .1\n",
        "m=.1\n",
        "M=1\n",
        "g=9.8\n",
        "l=.5\n",
        "A=jnp.array([[0,1,0,0],\n",
        "             [0,0,m*g*delta_t/M,0],\n",
        "             [0,0,0,1],\n",
        "             [0,0,(m+M)*g*delta_t/(M*l),0]])\n",
        "\n",
        "#A=jnp.array([[1,delta_t,0,0],\n",
        "#             [0,1,m*g*delta_t/M,0],\n",
        "#             [0,0,1,delta_t],\n",
        "#             [0,0,(m+M)*g*delta_t/(M*l),1]])\n",
        "\n",
        "B=jnp.array([[0],[delta_t/M],[0],[delta_t/(M*l)]])\n",
        "\n",
        "H=jnp.array([[0,1,0,0],\n",
        "             [0,0,0,1]])\n",
        "\n",
        "R=jnp.eye(H.shape[0])*0.005\n",
        "C=H\n",
        "Q=jnp.eye(A.shape[0])*0.005\n",
        "mean=0\n",
        "std_dev=0.005\n",
        "#Q=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean     #FAILED\n",
        "P_0=jnp.ones(A.shape)\n",
        "key=jrandom.PRNGKey(42)\n",
        "Z=jrandom.normal(key, shape=(C.shape[0],1)) * std_dev + mean\n",
        "w_k=jnp.ones((A.shape[0],1))*0.005\n",
        "#w_k=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean    #FAILED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 20.3     |\n",
            "|    ep_rew_mean     | 20.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 150      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 26.9        |\n",
            "|    ep_rew_mean          | 26.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 104         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 39          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010784712 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.685      |\n",
            "|    explained_variance   | 0.00019     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.57        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    value_loss           | 45.6        |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m kalman_env \u001b[38;5;241m=\u001b[39m KalmanRLWrapper(env, kf)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, kalman_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "kalman_env = KalmanRLWrapper(env, kf)\n",
        "model = PPO(\"MlpPolicy\", kalman_env, verbose=1)\n",
        "model.learn(total_timesteps=500_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 1000.0\n",
            "Episode: 2, Rewards: 1000.0\n",
            "Episode: 3, Rewards: 1000.0\n",
            "Episode: 4, Rewards: 1000.0\n",
            "Episode: 5, Rewards: 1000.0\n",
            "Episode: 6, Rewards: 1000.0\n",
            "Episode: 7, Rewards: 1000.0\n",
            "Episode: 8, Rewards: 1000.0\n",
            "Episode: 9, Rewards: 1000.0\n",
            "Episode: 10, Rewards: 1000.0\n",
            "mean_rewards: 1000.0 (100.0 %)\n"
          ]
        }
      ],
      "source": [
        "model=PPO.load(\"ppo_cartpole_100k_noKF\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model.predict(state)\n",
        "        state, reward, done, truncuated, info = env.step(action)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 885.0\n",
            "Episode: 2, Rewards: 1000.0\n",
            "Episode: 3, Rewards: 1000.0\n",
            "Episode: 4, Rewards: 1000.0\n",
            "Episode: 5, Rewards: 1000.0\n",
            "Episode: 6, Rewards: 1000.0\n",
            "Episode: 7, Rewards: 1000.0\n",
            "Episode: 8, Rewards: 1000.0\n",
            "Episode: 9, Rewards: 1000.0\n",
            "Episode: 10, Rewards: 1000.0\n",
            "mean_rewards: 988.5 (98.85 %)\n"
          ]
        }
      ],
      "source": [
        "model1=PPO.load(\"ppo_cartpole_500k\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "kalman_env = KalmanRLWrapper(env, kf)\n",
        "P=P_0\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    state_p = jnp.expand_dims(kalman_env.reset()[0], axis=-1)\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model1.predict(state_p.reshape(1,4).squeeze())\n",
        "        action2= -1 if action == 0 else 1\n",
        "        u_k = jnp.array([[action2]])\n",
        "        state_p=step_estimation(A,B,state_p,u_k,w_k)\n",
        "        P=process_covariance(A,P,Q)\n",
        "        \n",
        "        state_m, reward, done, truncuated, info = kalman_env.step(action)\n",
        "        K=kalman_function(P,H,R)\n",
        "        state_p,P=current_state_and_process(K,H,C,state_m,state_p,P,Z)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 12.0\n",
            "Episode: 2, Rewards: 11.0\n",
            "Episode: 3, Rewards: 12.0\n",
            "Episode: 4, Rewards: 13.0\n",
            "Episode: 5, Rewards: 12.0\n",
            "Episode: 6, Rewards: 10.0\n",
            "Episode: 7, Rewards: 12.0\n",
            "Episode: 8, Rewards: 11.0\n",
            "Episode: 9, Rewards: 12.0\n",
            "Episode: 10, Rewards: 11.0\n",
            "mean_rewards: 11.6 (1.16 %)\n"
          ]
        }
      ],
      "source": [
        "model1=PPO.load(\"ppo_cartpole_500k\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "    kalman_env = KalmanRLWrapper(env, kf)\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model1.predict(kalman_env.kf.x_k.squeeze())\n",
        "        action2= -1 if action == 0 else 1\n",
        "        u_k = jnp.array([[action2]])\n",
        "        #kf.predict(u_k)\n",
        "        kalman_env.kf.predict(u_k)\n",
        "        state_m, reward, done, truncuated, info = kalman_env.step(action)\n",
        "        #kf.update(state_m)\n",
        "        kalman_env.kf.update(state_m)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
