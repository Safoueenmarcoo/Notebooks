{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import gymnasium as gym\n",
        "import jax.random as jrandom\n",
        "from stable_baselines3 import PPO\n",
        "from utils_v2 import KalmanFilter,KalmanRLWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_t = .1\n",
        "m=.1\n",
        "M=1\n",
        "g=9.8\n",
        "l=.5\n",
        "A=jnp.array([[0,1,0,0],\n",
        "             [0,0,m*g*delta_t/M,0],\n",
        "             [0,0,0,1],\n",
        "             [0,0,(m+M)*g*delta_t/(M*l),0]])\n",
        "\n",
        "#A=jnp.array([[1,delta_t,0,0],\n",
        "#             [0,1,m*g*delta_t/M,0],\n",
        "#             [0,0,1,delta_t],\n",
        "#             [0,0,(m+M)*g*delta_t/(M*l),1]])\n",
        "\n",
        "B=jnp.array([[0],[delta_t/M],[0],[delta_t/(M*l)]])\n",
        "\n",
        "H=jnp.array([[0,1,0,0],\n",
        "             [0,0,0,1]])\n",
        "\n",
        "R=jnp.eye(H.shape[0])*0.005\n",
        "C=H\n",
        "Q=jnp.eye(A.shape[0])*0.005\n",
        "mean=0\n",
        "std_dev=0.005\n",
        "#Q=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean     #FAILED\n",
        "P_0=jnp.ones(A.shape)\n",
        "key=jrandom.PRNGKey(42)\n",
        "Z=jrandom.normal(key, shape=(C.shape[0],1)) * std_dev + mean\n",
        "w_k=jnp.ones((A.shape[0],1))*0.005\n",
        "#w_k=jrandom.normal(key, shape=(A.shape[0],1)) * std_dev + mean    #FAILED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 22.3     |\n",
            "|    ep_rew_mean     | 22.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 226      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m kalman_env \u001b[38;5;241m=\u001b[39m KalmanRLWrapper(env, kf)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, kalman_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\hamed\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:230\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mth\u001b[38;5;241m.\u001b[39mmin(policy_loss_1, policy_loss_2)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m pg_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpolicy_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    231\u001b[0m clip_fraction \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mmean((th\u001b[38;5;241m.\u001b[39mabs(ratio \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m clip_range)\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    232\u001b[0m clip_fractions\u001b[38;5;241m.\u001b[39mappend(clip_fraction)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "kalman_env = KalmanRLWrapper(env, kf)\n",
        "model = PPO(\"MlpPolicy\", kalman_env, verbose=1)\n",
        "model.learn(total_timesteps=500_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 1000.0\n",
            "Episode: 2, Rewards: 1000.0\n",
            "Episode: 3, Rewards: 1000.0\n",
            "Episode: 4, Rewards: 1000.0\n",
            "Episode: 5, Rewards: 1000.0\n",
            "Episode: 6, Rewards: 1000.0\n",
            "Episode: 7, Rewards: 1000.0\n",
            "Episode: 8, Rewards: 1000.0\n",
            "Episode: 9, Rewards: 1000.0\n",
            "Episode: 10, Rewards: 1000.0\n",
            "mean_rewards: 1000.0 (100.0 %)\n"
          ]
        }
      ],
      "source": [
        "model=PPO.load(\"ppo_cartpole_100k_noKF\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model.predict(state)\n",
        "        state, reward, done, truncuated, info = env.step(action)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 885.0\n",
            "Episode: 2, Rewards: 1000.0\n",
            "Episode: 3, Rewards: 1000.0\n",
            "Episode: 4, Rewards: 1000.0\n",
            "Episode: 5, Rewards: 1000.0\n",
            "Episode: 6, Rewards: 1000.0\n",
            "Episode: 7, Rewards: 1000.0\n",
            "Episode: 8, Rewards: 1000.0\n",
            "Episode: 9, Rewards: 1000.0\n",
            "Episode: 10, Rewards: 1000.0\n",
            "mean_rewards: 988.5 (98.85 %)\n"
          ]
        }
      ],
      "source": [
        "model1=PPO.load(\"ppo_cartpole_500k\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "kalman_env = KalmanRLWrapper(env, kf)\n",
        "P=P_0\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    state_p = jnp.expand_dims(kalman_env.reset()[0], axis=-1)\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model1.predict(state_p.reshape(1,4).squeeze())\n",
        "        action2= -1 if action == 0 else 1\n",
        "        u_k = jnp.array([[action2]])\n",
        "        state_p=step_estimation(A,B,state_p,u_k,w_k)\n",
        "        P=process_covariance(A,P,Q)\n",
        "        \n",
        "        state_m, reward, done, truncuated, info = kalman_env.step(action)\n",
        "        K=kalman_function(P,H,R)\n",
        "        state_p,P=current_state_and_process(K,H,C,state_m,state_p,P,Z)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Rewards: 12.0\n",
            "Episode: 2, Rewards: 11.0\n",
            "Episode: 3, Rewards: 12.0\n",
            "Episode: 4, Rewards: 13.0\n",
            "Episode: 5, Rewards: 12.0\n",
            "Episode: 6, Rewards: 10.0\n",
            "Episode: 7, Rewards: 12.0\n",
            "Episode: 8, Rewards: 11.0\n",
            "Episode: 9, Rewards: 12.0\n",
            "Episode: 10, Rewards: 11.0\n",
            "mean_rewards: 11.6 (1.16 %)\n"
          ]
        }
      ],
      "source": [
        "model1=PPO.load(\"ppo_cartpole_500k\")\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env._max_episode_steps=1000\n",
        "\n",
        "rewardss = 0\n",
        "episodes = 10\n",
        "for ep in range(episodes):\n",
        "    kf = KalmanFilter(env.reset()[0],A,B,H,R,C,Q,Z,w_k,P_0)\n",
        "    kalman_env = KalmanRLWrapper(env, kf)\n",
        "    rewards = 0\n",
        "    while True:\n",
        "        action, _ = model1.predict(kalman_env.kf.x_k.squeeze())\n",
        "        action2= -1 if action == 0 else 1\n",
        "        u_k = jnp.array([[action2]])\n",
        "        #kf.predict(u_k)\n",
        "        kalman_env.kf.predict(u_k)\n",
        "        state_m, reward, done, truncuated, info = kalman_env.step(action)\n",
        "        #kf.update(state_m)\n",
        "        kalman_env.kf.update(state_m)\n",
        "        rewards += reward\n",
        "        if done or truncuated:\n",
        "            break\n",
        "    print(f\"Episode: {ep+1}, Rewards: {rewards}\")\n",
        "    rewardss += rewards\n",
        "mean=round(rewardss/(episodes),2)\n",
        "print(f\"mean_rewards: {mean} ({round(mean/env._max_episode_steps*100,2)} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
